{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark in Python using DataFrame API\n",
    "\n",
    "Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib and ML for machine learning, GraphX for graph processing, and Spark Streaming.\n",
    "\n",
    "This notebook uses a  machine learning problem based upon a dataset available at: http://www.sgi.com/tech/mlc/db/churn.data. \n",
    "\n",
    "In the targeted approach the company tries to identify in advance customers who are likely to churn. The company then targets those customers with special programs or incentives. This approach can bring in huge loss for a company, if churn predictions are inaccurate, because then firms are wasting incentive money on customers who would have stayed anyway. There are numerous predictive modeling techniques for predicting customer churn. The task here is to predict whether the customer will churn or not, using the given features.\n",
    "\n",
    "The data files state that the data are \"artificial based on claims similar to real world\". These data are also contained in the C50 R package. This analysis make use of the DataFrame based API based on the pyspark.ml library. Read more on DataFrame based Spark API at : http://spark.apache.org/docs/latest/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing the pyspark libraries and associated classes. We have imported the Pandas as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets build a Spark session using the session builder as below. Alternatively, we can create a SparkContext object.\n",
    "\n",
    "sc = pyspark.SparkContext( )\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start Spark session\n",
    "spark = pyspark.sql.SparkSession.builder.appName('Churn').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Spark session object is created, we now read in the dataset as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load churn_data.csv into Spark dataframe using Pandas\n",
    "df = spark.createDataFrame(pd.read_csv('churn_data.csv', names=[\"state\",\"account_length\",\"area_code\",\"phone_number\",\"international_plan\",\"voice_mail_plan\",\"number_vmail_messages\",\"total_day_minutes\",\"total_day_calls\",\"total_day_charge\",\n",
    "                                                                \"total_eve_minutes\",\"total_eve_calls\",\"total_eve_charge\",\"total_night_minutes\",\"total_night_calls\",\"total_night_charge\",\"total_intl_minutes\",\"total_intl_calls\",\"total_intl_charge\",\n",
    "                                                                \"number_customer_service_calls\",\"churned\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets view the first few rows of the DataFrame, df so created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+---------+------------+------------------+---------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+\n",
      "|state|account_length|area_code|phone_number|international_plan|voice_mail_plan|number_vmail_messages|total_day_minutes|total_day_calls|total_day_charge|total_eve_minutes|total_eve_calls|total_eve_charge|total_night_minutes|total_night_calls|total_night_charge|total_intl_minutes|total_intl_calls|total_intl_charge|number_customer_service_calls|churned|\n",
      "+-----+--------------+---------+------------+------------------+---------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+\n",
      "|   KS|           128|      415|    382-4657|                no|            yes|                   25|            265.1|            110|           45.07|            197.4|             99|           16.78|              244.7|               91|             11.01|              10.0|               3|              2.7|                            1| False.|\n",
      "|   OH|           107|      415|    371-7191|                no|            yes|                   26|            161.6|            123|           27.47|            195.5|            103|           16.62|              254.4|              103|             11.45|              13.7|               3|              3.7|                            1| False.|\n",
      "|   NJ|           137|      415|    358-1921|                no|             no|                    0|            243.4|            114|           41.38|            121.2|            110|            10.3|              162.6|              104|              7.32|              12.2|               5|             3.29|                            0| False.|\n",
      "|   OH|            84|      408|    375-9999|               yes|             no|                    0|            299.4|             71|            50.9|             61.9|             88|            5.26|              196.9|               89|              8.86|               6.6|               7|             1.78|                            2| False.|\n",
      "|   OK|            75|      415|    330-6626|               yes|             no|                    0|            166.7|            113|           28.34|            148.3|            122|           12.61|              186.9|              121|              8.41|              10.1|               3|             2.73|                            3| False.|\n",
      "+-----+--------------+---------+------------+------------------+---------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now choose relevant input features for our analysis, with \"churned\" as the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[\"account_length\",\"number_vmail_messages\",\"total_day_minutes\",\"total_day_calls\",\"total_day_charge\",\"total_eve_minutes\",\"total_eve_calls\",\"total_eve_charge\",\"total_night_minutes\",\"total_night_calls\",\"total_night_charge\",\"total_intl_minutes\",\"total_intl_calls\",\"total_intl_charge\",\"number_customer_service_calls\",\"churned\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account_length: long (nullable = true)\n",
      " |-- number_vmail_messages: long (nullable = true)\n",
      " |-- total_day_minutes: double (nullable = true)\n",
      " |-- total_day_calls: long (nullable = true)\n",
      " |-- total_day_charge: double (nullable = true)\n",
      " |-- total_eve_minutes: double (nullable = true)\n",
      " |-- total_eve_calls: long (nullable = true)\n",
      " |-- total_eve_charge: double (nullable = true)\n",
      " |-- total_night_minutes: double (nullable = true)\n",
      " |-- total_night_calls: long (nullable = true)\n",
      " |-- total_night_charge: double (nullable = true)\n",
      " |-- total_intl_minutes: double (nullable = true)\n",
      " |-- total_intl_calls: long (nullable = true)\n",
      " |-- total_intl_charge: double (nullable = true)\n",
      " |-- number_customer_service_calls: long (nullable = true)\n",
      " |-- churned: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3333"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe now has 3333 rows of features. Now lets split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data, test_data = df.randomSplit([0.7, 0.3], seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Pipelining\n",
    "\n",
    "Lets work with the Spark DataFrames by making use of the extensive pipelining feature available. Pipelining of DataFrames involves the following stages. Lets move on to them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Converting categorical attribute label indexes into numeric\n",
    "\n",
    "2.Encoding numerics into numerical vectors\n",
    "\n",
    "3.Combining all numerical vectors into a single feature vector and making the pipeline\n",
    "\n",
    "4.Fitting a ML model on the extracted features\n",
    "\n",
    "5.Predicting using ML model to get outputs for each data row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Converting categorical attribute labels into label indexes\n",
    "\n",
    "This is done making use of the StringIndexer class, which converts the categorical output into numerics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+---------------+\n",
      "|account_length|number_vmail_messages|total_day_minutes|total_day_calls|total_day_charge|total_eve_minutes|total_eve_calls|total_eve_charge|total_night_minutes|total_night_calls|total_night_charge|total_intl_minutes|total_intl_calls|total_intl_charge|number_customer_service_calls|churned|churned_numeric|\n",
      "+--------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+---------------+\n",
      "|             6|                    0|            183.6|            117|           31.21|            256.7|             72|           21.82|              178.6|               79|              8.04|              10.2|               2|             2.75|                            1| False.|            0.0|\n",
      "|            10|                    0|            186.1|            112|           31.64|            190.2|             66|           16.17|              282.8|               57|             12.73|              11.4|               6|             3.08|                            2| False.|            0.0|\n",
      "|            13|                   31|            265.3|             94|            45.1|            147.6|             95|           12.55|              259.3|              117|             11.67|              12.9|               1|             3.48|                            1| False.|            0.0|\n",
      "|            16|                    0|            229.6|             78|           39.03|            205.7|            108|           17.48|              166.2|               91|              7.48|              10.8|               2|             2.92|                            0|  True.|            1.0|\n",
      "|            18|                    0|            273.6|             93|           46.51|            114.6|            116|            9.74|              250.6|              120|             11.28|               8.2|               4|             2.21|                            1| False.|            0.0|\n",
      "+--------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol='churned', outputCol='churned_numeric').fit(df)\n",
    "indexed_df = indexer.transform(training_data)\n",
    "indexed_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Encoding numerics into numerical vectors\n",
    "\n",
    "This is done making use of the OneHotEncoder class, which converts the numerics into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+---------------+--------------+\n",
      "|account_length|number_vmail_messages|total_day_minutes|total_day_calls|total_day_charge|total_eve_minutes|total_eve_calls|total_eve_charge|total_night_minutes|total_night_calls|total_night_charge|total_intl_minutes|total_intl_calls|total_intl_charge|number_customer_service_calls|churned|churned_numeric|churned_vector|\n",
      "+--------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+---------------+--------------+\n",
      "|             6|                    0|            183.6|            117|           31.21|            256.7|             72|           21.82|              178.6|               79|              8.04|              10.2|               2|             2.75|                            1| False.|            0.0| (1,[0],[1.0])|\n",
      "|            10|                    0|            186.1|            112|           31.64|            190.2|             66|           16.17|              282.8|               57|             12.73|              11.4|               6|             3.08|                            2| False.|            0.0| (1,[0],[1.0])|\n",
      "|            13|                   31|            265.3|             94|            45.1|            147.6|             95|           12.55|              259.3|              117|             11.67|              12.9|               1|             3.48|                            1| False.|            0.0| (1,[0],[1.0])|\n",
      "|            16|                    0|            229.6|             78|           39.03|            205.7|            108|           17.48|              166.2|               91|              7.48|              10.8|               2|             2.92|                            0|  True.|            1.0|     (1,[],[])|\n",
      "|            18|                    0|            273.6|             93|           46.51|            114.6|            116|            9.74|              250.6|              120|             11.28|               8.2|               4|             2.21|                            1| False.|            0.0| (1,[0],[1.0])|\n",
      "+--------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"churned_numeric\", outputCol=\"churned_vector\")\n",
    "encoded_df = encoder.transform(indexed_df)\n",
    "encoded_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Combining all numerical vectors into a single feature vector\n",
    "\n",
    "This is done using the VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+---------------+--------------+--------------------+\n",
      "|account_length|number_vmail_messages|total_day_minutes|total_day_calls|total_day_charge|total_eve_minutes|total_eve_calls|total_eve_charge|total_night_minutes|total_night_calls|total_night_charge|total_intl_minutes|total_intl_calls|total_intl_charge|number_customer_service_calls|churned|churned_numeric|churned_vector|            features|\n",
      "+--------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+---------------+--------------+--------------------+\n",
      "|             6|                    0|            183.6|            117|           31.21|            256.7|             72|           21.82|              178.6|               79|              8.04|              10.2|               2|             2.75|                            1| False.|            0.0| (1,[0],[1.0])|[1.0,6.0,0.0,183....|\n",
      "|            10|                    0|            186.1|            112|           31.64|            190.2|             66|           16.17|              282.8|               57|             12.73|              11.4|               6|             3.08|                            2| False.|            0.0| (1,[0],[1.0])|[1.0,10.0,0.0,186...|\n",
      "|            13|                   31|            265.3|             94|            45.1|            147.6|             95|           12.55|              259.3|              117|             11.67|              12.9|               1|             3.48|                            1| False.|            0.0| (1,[0],[1.0])|[1.0,13.0,31.0,26...|\n",
      "|            16|                    0|            229.6|             78|           39.03|            205.7|            108|           17.48|              166.2|               91|              7.48|              10.8|               2|             2.92|                            0|  True.|            1.0|     (1,[],[])|[0.0,16.0,0.0,229...|\n",
      "|            18|                    0|            273.6|             93|           46.51|            114.6|            116|            9.74|              250.6|              120|             11.28|               8.2|               4|             2.21|                            1| False.|            0.0| (1,[0],[1.0])|[1.0,18.0,0.0,273...|\n",
      "+--------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+---------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['churned_vector', 'account_length',\n",
    "              \"number_vmail_messages\",\"total_day_minutes\",\"total_day_calls\",\"total_day_charge\",\n",
    "               \"total_eve_minutes\",\"total_eve_calls\",\"total_eve_charge\",\"total_night_minutes\",\"total_night_calls\",\n",
    "               \"total_night_charge\",\"total_intl_minutes\",\"total_intl_calls\",\"total_intl_charge\",\"number_customer_service_calls\"], \n",
    "    outputCol=\"features\")\n",
    "\n",
    "final_df = assembler.transform(encoded_df)\n",
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the pipeline\n",
    "Lets now build the pipeline , combining all the above three stages, on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+---------------+--------------+--------------------+\n",
      "|account_length|number_vmail_messages|total_day_minutes|total_day_calls|total_day_charge|total_eve_minutes|total_eve_calls|total_eve_charge|total_night_minutes|total_night_calls|total_night_charge|total_intl_minutes|total_intl_calls|total_intl_charge|number_customer_service_calls|churned|churned_numeric|churned_vector|            features|\n",
      "+--------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+---------------+--------------+--------------------+\n",
      "|             6|                    0|            183.6|            117|           31.21|            256.7|             72|           21.82|              178.6|               79|              8.04|              10.2|               2|             2.75|                            1| False.|            0.0| (1,[0],[1.0])|[1.0,6.0,0.0,183....|\n",
      "|            10|                    0|            186.1|            112|           31.64|            190.2|             66|           16.17|              282.8|               57|             12.73|              11.4|               6|             3.08|                            2| False.|            0.0| (1,[0],[1.0])|[1.0,10.0,0.0,186...|\n",
      "|            13|                   31|            265.3|             94|            45.1|            147.6|             95|           12.55|              259.3|              117|             11.67|              12.9|               1|             3.48|                            1| False.|            0.0| (1,[0],[1.0])|[1.0,13.0,31.0,26...|\n",
      "|            16|                    0|            229.6|             78|           39.03|            205.7|            108|           17.48|              166.2|               91|              7.48|              10.8|               2|             2.92|                            0|  True.|            1.0|     (1,[],[])|[0.0,16.0,0.0,229...|\n",
      "|            18|                    0|            273.6|             93|           46.51|            114.6|            116|            9.74|              250.6|              120|             11.28|               8.2|               4|             2.21|                            1| False.|            0.0| (1,[0],[1.0])|[1.0,18.0,0.0,273...|\n",
      "+--------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+---------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[indexer, encoder, assembler])\n",
    "pipelined_model = pipeline.fit(training_data)\n",
    "pipelined_training_data = pipelined_model.transform(training_data)\n",
    "pipelined_training_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Fitting the Logistic Regression model\n",
    "\n",
    "Lets now fit the Logistic Regression model using the input features and labels. We also apply some regularization parameters to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"churned_numeric\", featuresCol=\"features\",\n",
    "                        maxIter=10, regParam=0.3, elasticNetParam=0.8,family = 'multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "lrModel = lr.fit(pipelined_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the coefficients and intercepts for the two-class logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients: 2 X 16 CSRMatrix\n",
      "(0,0) 0.7936\n",
      "(1,0) -0.7936\n",
      "intercepts: [0.268766962696,-0.268766962696]\n"
     ]
    }
   ],
   "source": [
    "print(\"coefficients: \" + str(lrModel.coefficientMatrix))\n",
    "print(\"intercepts: \" + str(lrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the predictions\n",
    "\n",
    "We now make predictions using the transformed test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+---------------+--------------+--------------------+\n",
      "|account_length|number_vmail_messages|total_day_minutes|total_day_calls|total_day_charge|total_eve_minutes|total_eve_calls|total_eve_charge|total_night_minutes|total_night_calls|total_night_charge|total_intl_minutes|total_intl_calls|total_intl_charge|number_customer_service_calls|churned|churned_numeric|churned_vector|            features|\n",
      "+--------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+---------------+--------------+--------------------+\n",
      "|             3|                   36|            118.1|            117|           20.08|            221.5|            125|           18.83|              103.9|               89|              4.68|              11.9|               6|             3.21|                            2| False.|            0.0| (1,[0],[1.0])|[1.0,3.0,36.0,118...|\n",
      "|            11|                   28|            190.6|             86|            32.4|            220.1|            122|           18.71|              180.3|               80|              8.11|               6.0|               3|             1.62|                            3| False.|            0.0| (1,[0],[1.0])|[1.0,11.0,28.0,19...|\n",
      "|            12|                    0|            249.6|            118|           42.43|            252.4|            119|           21.45|              280.2|               90|             12.61|              11.8|               3|             3.19|                            1|  True.|            1.0|     (1,[],[])|[0.0,12.0,0.0,249...|\n",
      "|            13|                   21|            315.6|            105|           53.65|            208.9|             71|           17.76|              260.1|              123|              11.7|              12.1|               3|             3.27|                            3| False.|            0.0| (1,[0],[1.0])|[1.0,13.0,21.0,31...|\n",
      "|            16|                    0|            205.6|             69|           34.95|            169.5|             93|           14.41|              220.1|               64|               9.9|              10.9|               3|             2.94|                            0| False.|            0.0| (1,[0],[1.0])|[1.0,16.0,0.0,205...|\n",
      "+--------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+-----------------------------+-------+---------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "pipelined_test_data = pipelined_model.transform(test_data)\n",
    "pipelined_test_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(pipelined_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above returns a prediction DataFrame containing the vector of predictions and actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------------+\n",
      "|prediction|churned_numeric|            features|\n",
      "+----------+---------------+--------------------+\n",
      "|       0.0|            0.0|[1.0,3.0,36.0,118...|\n",
      "|       0.0|            0.0|[1.0,11.0,28.0,19...|\n",
      "|       0.0|            1.0|[0.0,12.0,0.0,249...|\n",
      "|       0.0|            0.0|[1.0,13.0,21.0,31...|\n",
      "|       0.0|            0.0|[1.0,16.0,0.0,205...|\n",
      "+----------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\",\"churned_numeric\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make use of the Multiclass. Evaluator to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.8461538461538461\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = 'churned_numeric',\n",
    "                                              predictionCol = 'prediction',\n",
    "                                              metricName = 'accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "Now lets perform a simple cross validation using the CrossValidator class. The parameters of the model built are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.8)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto, current: multinomial)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: churned_numeric)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the parameters regParam, elasticNetParam and maxIter for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(pipelined_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will likely take a fair amount of time because of the amount of models that we're creating and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(pipelined_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cvModel uses the best model found from the Cross Validation. Evaluate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999000999000999"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Concluding Remarks\n",
    "1. A Spark session is created, data is read in and transformed using ML pipelining \n",
    "2. A Machine Learning model is created, predictions are made on the test data and evaluated accuracies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
